import os
import time
import pandas as pd
from datetime import datetime
from openai import OpenAI
from dotenv import load_dotenv

# 1ï¸âƒ£ í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ì½ê¸°
load_dotenv()
API_KEY = os.getenv("API_KEY") or os.getenv("OPENAI_API_KEY")
if not API_KEY or "your-api-key" in API_KEY:
    print("âŒ API í‚¤ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
    exit()

client = OpenAI(api_key=API_KEY)

# 2ï¸âƒ£ í…ŒìŠ¤íŠ¸ ì„¤ì •
MODEL = "gpt-5"
TEST_QUERY = "ì„œìš¸ì‹œ ì¢…ë¡œêµ¬ ë§›ì§‘ ì¶”ì²œ"

SEARCH_TYPES = {
    "non_reasoning": {"reasoning": {"effort": "low"}},
    "agentic": {"reasoning": {"effort": "medium"}},
    "deep_research": {"reasoning": {"effort": "high"}}
}

# 3ï¸âƒ£ ì‘ë‹µ ì¶”ì¶œ í•¨ìˆ˜
def extract_response_text(response):
    try:
        if hasattr(response, 'output_text'):
            return response.output_text or ""
        if hasattr(response, 'output'):
            for out in response.output:
                for content in out.content:
                    if hasattr(content, 'text'):
                        return content.text
        return ""
    except:
        return ""

def extract_citations(response):
    urls, titles = [], []
    try:
        if hasattr(response, 'output'):
            for out in response.output:
                for content in out.content:
                    if hasattr(content, 'annotations'):
                        for ann in content.annotations:
                            if getattr(ann, 'type', '') == "url_citation":
                                urls.append(getattr(ann, 'url', ''))
                                titles.append(getattr(ann, 'title', ''))
        return urls, titles
    except:
        return [], []

# 4ï¸âƒ£ ì›¹ ê²€ìƒ‰ ì‹¤í–‰ í•¨ìˆ˜
def run_web_search(query, search_type_name, reasoning_params):
    print(f"ğŸ”¹ [{search_type_name}] ê²€ìƒ‰ ì‹œì‘...")
    start_time = time.time()
    
    response = client.responses.create(
        model=MODEL,
        tools=[{"type": "web_search"}],
        reasoning=reasoning_params,
        input=query
    )
    
    duration = time.time() - start_time
    response_text = extract_response_text(response)
    urls, titles = extract_citations(response)
    
    result = {
        "search_type": search_type_name,
        "query": query,
        "duration_seconds": round(duration, 2),
        "response_text": response_text,
        "citation_count": len(urls),
        "citation_urls": urls,
        "citation_titles": titles
    }
    print(f"âœ… [{search_type_name}] ì™„ë£Œ! ({round(duration,2)}ì´ˆ)")
    return result

# 5ï¸âƒ£ ê²°ê³¼ ì €ì¥ í•¨ìˆ˜
def save_results(results):
    df = pd.DataFrame(results)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    csv_file = f"gpt5_web_search_results_{timestamp}.csv"
    excel_file = f"gpt5_web_search_results_{timestamp}.xlsx"
    
    df.to_csv(csv_file, index=False, encoding='utf-8-sig')
    with pd.ExcelWriter(excel_file, engine='xlsxwriter') as writer:
        df.to_excel(writer, sheet_name='Results', index=False)
    
    return csv_file, excel_file

# 6ï¸âƒ£ ë©”ì¸ ì‹¤í–‰
def main():
    results = []
    for name, params in SEARCH_TYPES.items():
        res = run_web_search(TEST_QUERY, name, params["reasoning"])
        results.append(res)
        print("\nì¶œë ¥ ì˜ˆì‹œ:", res["response_text"][:300], "...")  # ìµœëŒ€ 300ì
    
    csv_path, excel_path = save_results(results)
    print(f"\n ê²°ê³¼ ì €ì¥:\n- CSV: {csv_path}\n- Excel: {excel_path}")

if __name__ == "__main__":
    main()